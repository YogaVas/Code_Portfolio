---
title             : "**Code Portfolio - EDA using R**"
shorttitle        : "Exploratory Data Analysis - Consolidated Topics by R"
author: 
  - name          : "Yogananth Vasudevan"
    affiliation   : "Harrisburg University"
    corresponding : yes
    address       : "326 Market St, Harrisburg, PA 17101"
    email         : "YVasudevan@my.harrisburgu.edu"
affiliation:
  - id            : "1"
    institution   : "Harrisburg University"
authornote: |
  **Student: Yogananth Vasudevan** --
  **Professor: Dr.Olga Schrivner - Exploratory Data Analysis, Department of Analytics**
bibliography      : ["r-references.bib"]
floatsintext      : no
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : no
mask              : no
draft             : no
documentclass     : "apa6"
classoption       : "man"
output            : 
 pdf_document: papaja::apa6_pdf
 html_document: default
---

```{r setup, include = FALSE}
library("papaja")
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```
#------------------------------------------------------------------------------
#Part1: Data Import,Workspace management & Wrangling (Data Manipulation)
#------------------------------------------------------------------------------

In this section, we will be looking at the basic but yet more important stage of R programming called **Input** as this is how we bring different types of data into R. Not only we will look at importing .txt or .csv files but also how can we write them into a table or an excel file. Also, we will supeficially getinto workspace management, which comes in very handy to not lose the path or even manage the directory well.

In between the read -write of files and dataset, we will also look through some of the useful data manipulation and cleaning technique. The first and foremost is to see whether the data is **tidy** or not. A tidy dataset means *Each observation forms its own row* and *Each variable is saved in its own column* 

```{r echo= FALSE}
#Following packages are called from the library for data import functions
library(tidyverse) #Package called to use the readr package for INPUT step
library(xlsx) #Package called to export a dataset into an Excel file
library(dplyr) # This packages is spacial where it can be used for data definition check at hight level

```

```{r echo= FALSE}
#Managing the Workspace Directory
# For a beginner, it takes time to get used to the default directory or even for an expert, it comes in handy to check the directory often to ensure where the files are stored

getwd() #Displays the default directory
setwd(dir= "C:/Users/Yogananth/Documents")
list.files() #To get a list of all files in the current directory

#Data Import

#Read CSV
Profit <- read.csv("C:/Users/Yogananth/Documents/BivariateDataset.csv")

str(Profit) #Displays the data types of all variables in the dataset

#subset

Pro_Sub <- select(Profit, starts_with("T")) #select() is used to subset a dataset based on variables
subset(Pro_Sub) 

#Read delimited file

#Belowfn reads delimited(tab here) file from a diferent location by defining the arguments
Stats_Houses <- read.delim("C:/Mine/MultiRegA.txt",sep = "\t", header = TRUE)
head(Stats_Houses) #similar to TOP command in SQL

PriceOver470K <- filter(Stats_Houses, Price > 470000) #Useful data manipulation to filter
str(PriceOver470K) #43 rows fetched for Proce over $470K

#Arrange - Sorting a dataset helps in observing a dataset at a high level

mycars <- arrange(mtcars, cyl)#Here we created a new df with mtcars cending order by cyl
mycars1 <- arrange(mtcars, desc(cyl))

#Combine - Combining vectors

data_frame(x=1:8, y = 9:16)

#Create New Variables & group by category

mutate(iris, Sepal = Sepal.Length + Sepal.Width )
group_by(iris, Species)
ungroup(iris)


#Missing Values - na.rm function coupled with mean after analyzing individual column and deciding whethere mean as a replacement will be the closest representation of the data needs to be done.

for(i in 3:ncol(mtcars)){
  mtcars[is.na(mtcars[,i]), i] <- mean(data.matrix(mtcars[,i]), na.rm = TRUE)
}

#To read just Tab delimited files, we can use the following

Stats_H_New <- read_tsv("C:/Mine/MultiRegA.txt",col_names = TRUE, 
                           col_types= NULL)
#Parsed with column specification: Output
#cols(
 # Price = col_double(),
 # Size = col_double(),
 # Tax = col_double(),
 # Bedroom = col_double() )

# Writing a dataset from studio to an excel or txt file

write.xlsx(mtcars, "C:/Users/Yogananth/Documents/mtcars.xlsx") #Excel file

write.table(mtcars, 
              "C:/Users/Yogananth/Documents/mtcars_N.txt", sep="\t") #Delimited(tab)
```

#------------------------------------------------------------------------------
#Part2: Data Visualization
#------------------------------------------------------------------------------

Data visualization in R is on par with any other Analytics programming or visualization tools with the following aspects in terms of needs:

      ## Exploratory Graphs
      ## Plotting systems
      
In this section, we will look at both of the plotting options that help any research initiative to draw inference out of the analysis by both statistical output and ric graphical output using ggplot. A graph is made up of many layers and each layer is transparent and stacked one after the other and each layer could be text, data points, lines, bars, pictures, etc. A visual element as a layer is called geoms in ggplot. The visual element is modified as per the desired output of a graph by using a function called aes(), which is nothing but aesthetics, by using aes() function aesthetical properties can be modified as per the need.

    ```{r echo = FALSE}
    #install.packages("ggplot2")
#Here we will get the frequency distribution using conventional exploratory graph for Histogram as well try the same with ggplot
   getwd()
   SurveyData <- read.csv(file="C:/Users/Yogananth/Documents/Survey.csv", 
                           header=TRUE, 
                         sep=",") #U
   #SurveyData
   library(ggplot2)
#Frequency Distribution of Quarter 1 using ggplot
   Parking_Histogram <- ggplot(SurveyData, aes(Q1))
   Parking_Histogram + geom_histogram(binwidth = 0.8)
#Frequency Distribution of Quarter 1 using Hist function
    hist(SurveyData$Q1, probability = TRUE, 
         col=grey(0.8),main="normal mu = 500 sigma = 50")
   
#Linear Model (Correlation Versus Regression) using Exploratory Graph functions

    Lcapacity = read.csv("C:/Users/Yogananth/Documents/LungCapData.csv")
    #str(Lcapacity)
    attach(Lcapacity)
 #Here it gives a closer look at the leverage versus residuals along with correlation between variables
    cor(Age, LungCap)
    plot(Age, LungCap)

    Model <- lm(LungCap ~ Age)
    par(mfrow=c(2,2)); plot(Model)    
    
#Same linear model analysis by ggplot  
    
  Lcapacity <- ggplot(Lcapacity,aes(LungCap, Age)) + 
               geom_point() + #Scatter points(SC Plot)
               geom_smooth() + #Add linearity to the plot
                labs(x= "Lung Capacity", y= "Age") #Lables

  #Boxplots - Let us look at the Boxplot using ggplot which gives the quantiles, IQR Mediam and mostimportantly Outliers           
  
Travel <- ggplot(SurveyData, aes(x = Gender, y = Q1, color = Gender)) +
  geom_boxplot()

Travel

Time <- ggplot(SurveyData, aes(x = Gender, y = Q2, color = Gender)) +
  geom_boxplot()

Time

Struck <- ggplot(SurveyData, aes(x = Gender, y = Q3, color = Gender)) +
  geom_boxplot()

Struck 
    
#BoxPlots & Density Plots

RehabAnalysis <- read.csv("C:/Users/Yogananth/Documents/Recstudy.csv")
#In the belwo box plot, we are trying to study the recovery time between two groups, one is the group that took the drug and the other being the placebo group. 

library("ggpubr")
ggboxplot(RehabAnalysis, x = "Groups", y = "Recdays", 
          color = "Groups", fill = "White",title = "Rehab Time Analysis", 
          palette = c("#00AFBB", "#E69F00"),linetype = "solid", 
          size = NULL, width = 0.7,
          ylab = "Recdays", xlab = "Groups")

   
    ```

#------------------------------------------------------------------------------
#Part3: Clustering & Principal COmponent Analysis
#------------------------------------------------------------------------------

Why PCA? PCA is a very useful analysis feature that helps when the dataset is rather wise and long. In other words, most of the real world datasets are large with respec to the number of variables that means it is very difficult understand the overall **shape** of the dataset, which in turn helps us to easily mark down any trend that is with in.

Basically, we take a dataset that is large with numerous variables and by using PCA, the dataset gets simplified by transforming the original variables into a smaller **Principal Components**. Or in other words, it is also called linear transformation.

```{r echo=FALSE}

#Here the PCA is done only with the numerica data and hence the categorical variables are omitted 

PCA_Cars <- prcomp(mtcars[,c(1:5,8,9)], center = TRUE,scale. = TRUE)

#Summarize the transformed Dataframe
summary(PCA_Cars)

#Plotting PCA
library(devtools)
#install_github("vqv/ggbiplot")
#library(ggbiplot)
#ggbiplot(PCA_Cars)

#DF created for just 4 numerical columns of interest to be paired
PCA_Cars_Dim <- mtcars[, c(1,3,4,7)]

pairs(PCA_Cars_Dim,cex.labels = 2, 
      font.labels = 2) #Each pair corresponds to the diagonal of the column to infer

#Following Descriptive stats give us the Highly Positive Correlated and Highly Negative Correlated

cor(PCA_Cars_Dim, use = "complete.obs")
residue <- cor(PCA_Cars_Dim, method='pearson') #Have used the default pearson method

#Covariance Matrix and Diagonal Elements 
CovRes = cov(PCA_Cars_Dim)
diag(CovRes)#Spits out the diagonal variances of 4 variables.

#Now the descripive correlation stats can be made into a plot which is read based on color, size and gradients

library(corrplot)

corrplot(residue, type = "upper", order = "hclust",
         tl.col = "black", tl.srt = 45)

```

```{r echo = FALSE}
#Clustering: - Hierarchial Clustering 

#Hierarchial Clustering has two types :- One is Agglomerative and the other is Divisive

data("airquality")
head(airquality)
AQ <- na.omit(airquality)
AQ <- scale(airquality)

# This step is crucial in creating a dissimilarity matrix
EC <- dist(AQ, method = "euclidean")

# C clust used here using ward method default
HC<- hclust(EC, method = "ward.D2" )

# For the dendograms, we are cutting the cluster into 4 groups
grp <- cutree(HC, k = 4)

# Visualize using the Plot & cluster it as rectangles
plot(HC, cex = 0.7) 
rect.hclust(HC, k = 4, border = 2:3) 


```

```{r echo = FALSE}
#Clustering: -  K-Means Clustering(Partition)
#install.packages("factoextra")
library("factoextra")
fviz_nbclust(AQ, kmeans, method = "gap_stat")
KM_Partition <- kmeans(AQ, 4, nstart = 20)

# Visualize using the cluster function
library("factoextra")
fviz_cluster(KM_Partition, data = my_data, frame.type = "convex")+
  theme_minimal()
```


